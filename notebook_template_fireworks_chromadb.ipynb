{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zj_H2zt0DPL1"
      },
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/mongodb-developer/ai-rag-lab-notebooks/blob/main/notebook_template.ipynb)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P2glaYJeDPL3"
      },
      "source": [
        "[![Lab Documentation and Solutions](https://img.shields.io/badge/Lab%20Documentation%20and%20Solutions-purple)](https://mongodb-developer.github.io/ai-rag-lab/)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8JIboqgBDPL4"
      },
      "source": [
        "# Step 1: Install libraries\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rpypO-_YDPL5"
      },
      "outputs": [],
      "source": [
        "! pip install -qU pymongo datasets langchain fireworks-ai tiktoken sentence_transformers tqdm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DPe3ZU8KDPL6"
      },
      "source": [
        "# Step 2: Setup prerequisites\n",
        "\n",
        "Replace:\n",
        "\n",
        "- `<MONGODB_URI>` with your **MongoDB connection string**\n",
        "- `<FIREWORKS_API_KEY>` with your **Fireworks API key**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9o73qXv0DPL6"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from pymongo import MongoClient"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AuNB7M39DPL6"
      },
      "outputs": [],
      "source": [
        "# Retain the quotes (\"\") when pasting the URI\n",
        "MONGODB_URI = \"<MONGODB_URI>\"\n",
        "# Initialize a MongoDB Python client\n",
        "mongodb_client = MongoClient(MONGODB_URI, appname=\"devrel.workshop.rag\")\n",
        "# Check the connection to the server\n",
        "mongodb_client.admin.command(\"ping\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UtkxIpPfDPL7"
      },
      "outputs": [],
      "source": [
        "# Retain the quotes (\"\") when pasting the API key\n",
        "os.environ[\"FIREWORKS_API_KEY\"] = \"fw_3ZjcUpN5hEVNZEvkTNdTVK2v\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VOiDLDzaDPL7"
      },
      "source": [
        "# Step 3: Load the dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "orxS26MFDPL7"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from datasets import load_dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3-H0rN77DPL8"
      },
      "outputs": [],
      "source": [
        "data = load_dataset(\"mongodb/devcenter-articles\", split=\"train\", streaming=True)\n",
        "data_head = data.take(20)\n",
        "docs = pd.DataFrame(data_head).to_dict(\"records\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LkR6947ODPL8"
      },
      "outputs": [],
      "source": [
        "# Check the number of documents in the dataset\n",
        "len(docs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bB4jogtODPL8"
      },
      "outputs": [],
      "source": [
        "# Preview a document\n",
        "docs[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NGwT0np6DPL8"
      },
      "source": [
        "# Step 4: Chunk up the data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "igsF1eHWDPL9"
      },
      "outputs": [],
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from typing import Dict, List"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P7xWOOWsDPL9"
      },
      "outputs": [],
      "source": [
        "# Separators to split on\n",
        "separators = [\"\\n\\n\", \"\\n\", \" \", \"\", \"#\", \"##\", \"###\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FYYOIog4DPL9"
      },
      "source": [
        "ðŸ“š https://python.langchain.com/v0.1/docs/modules/data_connection/document_transformers/split_by_token/#tiktoken\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "120bPP0ODPL-"
      },
      "outputs": [],
      "source": [
        "# Use the `RecursiveCharacterTextSplitter` text splitter with the `cl100k_base` encoding\n",
        "# For text data, you typically want to keep 1-2 paragraphs (~200 tokens) in a single chunk\n",
        "# Chunk overlap of 15-20% of the chunk size is recommended\n",
        "# Pass the `separators` list above as an argument called `separators`\n",
        "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
        "    encoding_name=\"cl100k_base\", separators=separators, chunk_size=200, chunk_overlap=30\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gnCyafzZDPL-"
      },
      "source": [
        "ðŸ“š https://api.python.langchain.com/en/latest/character/langchain_text_splitters.character.RecursiveCharacterTextSplitter.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KUeBXHuJDPL-"
      },
      "outputs": [],
      "source": [
        "def get_chunks(doc: Dict, text_field: str) -> List[Dict]:\n",
        "    \"\"\"\n",
        "    Chunk up a document.\n",
        "\n",
        "    Args:\n",
        "        doc (Dict): Parent document to generate chunks from.\n",
        "        text_field (str): Text field to chunk.\n",
        "\n",
        "    Returns:\n",
        "        List[Dict]: List of chunked documents.\n",
        "    \"\"\"\n",
        "    # Extract the field to chunk from `doc`\n",
        "    text = doc[text_field]\n",
        "    # Split `text` using the appropriate method of the `RecursiveCharacterTextSplitter` class\n",
        "    # NOTE: `text` is a string\n",
        "    chunks = text_splitter.split_text(text)\n",
        "\n",
        "    # Iterate through `chunks` and for each chunk:\n",
        "    # 1. Create a shallow copy of `doc`, call it `temp`\n",
        "    # 2. Set the `text_field` field in `temp` to the content of the chunk\n",
        "    # 3. Append `temp` to `chunked_data`\n",
        "    chunked_data = []\n",
        "    for chunk in chunks:\n",
        "       temp = doc.copy()\n",
        "       temp[text_field]=chunk\n",
        "       chunked_data.append(temp)\n",
        "\n",
        "    return chunked_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZXWfhXPjDPL-"
      },
      "outputs": [],
      "source": [
        "split_docs = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zSHJbjMSDPL-"
      },
      "outputs": [],
      "source": [
        "# Iterate through `docs`, use the `get_chunks` function to chunk up the documents based on the \"body\" field, and add the list of chunked documents to `split_docs` initialized above.\n",
        "split_docs = []\n",
        "for doc in docs:\n",
        "    chunks = get_chunks(doc, \"body\")\n",
        "    split_docs.extend(chunks)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a-GsahPVDPL_"
      },
      "outputs": [],
      "source": [
        "# Check that the length of the list of chunked documents is greater than the length of `docs`\n",
        "len(split_docs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "drnJ01xSDPL_"
      },
      "outputs": [],
      "source": [
        "# Preview one of the items in split_docs- ensure that it is a Python dictionary\n",
        "split_docs[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dai3SS-HDPL_"
      },
      "source": [
        "# Step 5: Generate embeddings\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qny2ibaFDPL_"
      },
      "outputs": [],
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W08mE2NvDPL_"
      },
      "outputs": [],
      "source": [
        "# Load the `gte-small` model using the Sentence Transformers library\n",
        "embedding_model = SentenceTransformer(\"thenlper/gte-small\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GGeomz0TDPL_"
      },
      "source": [
        "ðŸ“š https://huggingface.co/thenlper/gte-small"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nLJ_qdJSDPL_"
      },
      "outputs": [],
      "source": [
        "# Define a function that takes a piece of text (`text`) as input, embeds it using the `embedding_model` instantiated above and returns the embedding as a list\n",
        "# An array can be converted to a list using the `tolist()` method\n",
        "def get_embedding(text: str) -> List[float]:\n",
        "    \"\"\"\n",
        "    Generate the embedding for a piece of text.\n",
        "\n",
        "    Args:\n",
        "        text (str): Text to embed.\n",
        "\n",
        "    Returns:\n",
        "        List[float]: Embedding of the text as a list.\n",
        "    \"\"\"\n",
        "    embedding = embedding_model.encode(text)\n",
        "    return embedding.tolist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MA8J9V7uDPMA"
      },
      "outputs": [],
      "source": [
        "embedded_docs = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BJUcOj9sDPMA"
      },
      "outputs": [],
      "source": [
        "# Add an `embedding` field to each dictionary in `split_docs`\n",
        "# The `embedding` field should correspond to the embedding of the value of the `body` field\n",
        "# Use the `get_embedding` function defined above to generate the embedding\n",
        "# Append the updated dictionaries to `embedded_docs` initialized above.\n",
        "for doc in tqdm(split_docs):\n",
        "    doc[\"embedding\"] = get_embedding(doc[\"body\"])\n",
        "    embedded_docs.append(doc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XGkvhJgDDPMA"
      },
      "outputs": [],
      "source": [
        "# Check that the length of `embedded_docs` is the same as that of `split_docs`\n",
        "len(embedded_docs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hU0DiFaTDPMA"
      },
      "source": [
        "# Step 6: Ingest data into MongoDB\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rzEwNoNBDPMA"
      },
      "outputs": [],
      "source": [
        "# Name of the database -- Change if needed or leave as is\n",
        "DB_NAME = \"mongodb_rag_lab\"\n",
        "# Name of the collection -- Change if needed or leave as is\n",
        "COLLECTION_NAME = \"knowledge_base\"\n",
        "# Name of the vector search index -- Change if needed or leave as is\n",
        "ATLAS_VECTOR_SEARCH_INDEX_NAME = \"vector_index\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5XlwdJutDPMA"
      },
      "source": [
        "ðŸ“š https://pymongo.readthedocs.io/en/stable/tutorial.html#getting-a-database\n",
        "\n",
        "ðŸ“š https://pymongo.readthedocs.io/en/stable/tutorial.html#getting-a-collection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "auS_6a1MDPMB"
      },
      "outputs": [],
      "source": [
        "# Connect to the collection defined above using the `mongodb_client` defined in Step 2\n",
        "collection = mongodb_client[DB_NAME][COLLECTION_NAME]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8mRkwpaRDPMB"
      },
      "outputs": [],
      "source": [
        "# Bulk delete all existing records from the collection defined above\n",
        "collection.delete_many({})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mSsRekl4DPMB"
      },
      "source": [
        "ðŸ“š https://pymongo.readthedocs.io/en/stable/examples/bulk.html#bulk-insert\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FWGJ7rMcDPMB"
      },
      "outputs": [],
      "source": [
        "# Bulk insert `embedded_docs` into the collection defined above -- should be a one-liner\n",
        "collection.insert_many(embedded_docs)\n",
        "\n",
        "print(\"Data ingestion into MongoDB completed\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4zsn_iT4DPMG"
      },
      "source": [
        "# Step 7: Create a vector search index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k4MbkHNmDPMG"
      },
      "outputs": [],
      "source": [
        "# Create vector index definition specifying:\n",
        "# path: Path to the embeddings field\n",
        "# numDimensions: Number of embedding dimensions- depends on the embedding model used\n",
        "# similarity: Similarity metric. One of cosine, euclidean, dotProduct.\n",
        "model = {\n",
        "    \"name\": ATLAS_VECTOR_SEARCH_INDEX_NAME,\n",
        "    \"type\": \"vectorSearch\",\n",
        "    \"definition\": {\n",
        "        \"fields\": [\n",
        "            {\n",
        "                \"type\": \"vector\",\n",
        "                \"path\": \"embedding\",\n",
        "                \"numDimensions\": 384,\n",
        "                \"similarity\": \"cosine\",\n",
        "            }\n",
        "        ]\n",
        "    },\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JVjV1mXnDPMH"
      },
      "source": [
        "ðŸ“š https://pymongo.readthedocs.io/en/stable/api/pymongo/collection.html#pymongo.collection.Collection.create_search_index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5nbaJwQ0DPMH"
      },
      "outputs": [],
      "source": [
        "# Create a vector search index with the above definition for the `collection` collection\n",
        "collection.create_search_index(model=model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zmw5g3MtDPMH"
      },
      "source": [
        "# Step 8: Perform semantic search on your data\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XQHagUgNDPMH"
      },
      "source": [
        "### Define a vector search function\n",
        "\n",
        "ðŸ“š https://www.mongodb.com/docs/atlas/atlas-vector-search/vector-search-stage/#fields\n",
        "\n",
        "ðŸ“š https://www.mongodb.com/docs/atlas/atlas-vector-search/vector-search-stage/#ann-examples (Refer to the \"Basic Example\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AD2q2WSvDPMH"
      },
      "outputs": [],
      "source": [
        "# Define a function to retrieve relevant documents for a user query using vector search\n",
        "def vector_search(user_query: str) -> List[Dict]:\n",
        "    \"\"\"\n",
        "    Retrieve relevant documents for a user query using vector search.\n",
        "\n",
        "    Args:\n",
        "    user_query (str): The user's query string.\n",
        "\n",
        "    Returns:\n",
        "    list: A list of matching documents.\n",
        "    \"\"\"\n",
        "\n",
        "    # Generate embedding for the `user_query` using the `get_embedding` function defined in Step 5\n",
        "    query_embedding = get_embedding(user_query)\n",
        "\n",
        "    # Define an aggregation pipeline consisting of a $vectorSearch stage, followed by a $project stage\n",
        "    # Set the number of candidates to 150 and only return the top 5 documents from the vector search\n",
        "    # In the $project stage, exclude the `_id` field and include only the `body` field and `vectorSearchScore`\n",
        "    # NOTE: Use variables defined previously for the `index`, `queryVector` and `path` fields in the $vectorSearch stage\n",
        "    pipeline = [\n",
        "    {\n",
        "        \"$vectorSearch\": {\n",
        "            \"index\": ATLAS_VECTOR_SEARCH_INDEX_NAME,\n",
        "            \"queryVector\": query_embedding,\n",
        "            \"path\": \"embedding\",\n",
        "            \"numCandidates\": 150,\n",
        "            \"limit\": 5\n",
        "        }\n",
        "    },\n",
        "    {\n",
        "        \"$project\": {\n",
        "            \"_id\": 0,\n",
        "            \"body\": 1,\n",
        "            \"score\": {\"$meta\": \"vectorSearchScore\"}\n",
        "        }\n",
        "    }\n",
        "    ]\n",
        "\n",
        "    # Execute the aggregation `pipeline` and store the results in `results`\n",
        "    results = collection.aggregate(pipeline)\n",
        "    return list(results)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tah8HYANDPMH"
      },
      "source": [
        "### Run vector search queries\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "arBwVXBIDPMH"
      },
      "outputs": [],
      "source": [
        "vector_search(\"What is MongoDB Atlas Search?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lz3-mnxXDPMI"
      },
      "outputs": [],
      "source": [
        "vector_search(\"What are triggers in MongoDB Atlas?\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a4OjpC5XDPMI"
      },
      "source": [
        "# ðŸ¦¹â€â™€ï¸ Combine pre-filtering with vector search"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NBuLKgmxDPMI"
      },
      "source": [
        "### Filter for documents where the content type is `Video`\n",
        "\n",
        "ðŸ“š https://www.mongodb.com/docs/atlas/atlas-vector-search/vector-search-type/#about-the-filter-type"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tH0wGZOeDPMI"
      },
      "outputs": [],
      "source": [
        "# Modify the vector search index `model` (from Step 7) to include the `metadata.contentType` field as a `filter` field\n",
        "model = {\n",
        "    \"name\": ATLAS_VECTOR_SEARCH_INDEX_NAME,\n",
        "    \"type\": \"vectorSearch\",\n",
        "    \"definition\": {\n",
        "        \"fields\": [\n",
        "            {\n",
        "                \"type\": \"vector\",\n",
        "                \"path\": \"embedding\",\n",
        "                \"numDimensions\": 384,\n",
        "                \"similarity\": \"cosine\"\n",
        "            },\n",
        "            {\"type\": \"filter\", \"path\": \"metadata.contentType\"}\n",
        "        ]\n",
        "    }\n",
        "  }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mzetmTJsDPMI"
      },
      "outputs": [],
      "source": [
        "# Update vector search index\n",
        "collection.update_search_index(\n",
        "    name=ATLAS_VECTOR_SEARCH_INDEX_NAME, definition=model[\"definition\"]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NWXtTdnYDPMI"
      },
      "outputs": [],
      "source": [
        "# Embed the user query\n",
        "query_embedding = get_embedding(\"What is MongoDB Atlas Search?\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rfH-6fI_DPMI"
      },
      "source": [
        "ðŸ“š https://www.mongodb.com/docs/atlas/atlas-vector-search/vector-search-stage/#ann-examples (Refer to the \"Filter Example\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mhjf0azVDPMJ"
      },
      "outputs": [],
      "source": [
        "# Modify the $vectorSearch stage of the aggregation pipeline defined previously to include a filter for documents where the `metadata.contentType` field has the value \"Video\"\n",
        "pipeline = [\n",
        "    {\n",
        "        \"$vectorSearch\": {\n",
        "            \"index\": ATLAS_VECTOR_SEARCH_INDEX_NAME,\n",
        "            \"path\": \"embedding\",\n",
        "            \"queryVector\": query_embedding,\n",
        "            \"numCandidates\": 150,\n",
        "            \"limit\": 5,\n",
        "            \"filter\": {\"metadata.contentType\": \"Video\"}\n",
        "        }\n",
        "    },\n",
        "    {\n",
        "        \"$project\": {\n",
        "            \"_id\": 0,\n",
        "            \"body\": 1,\n",
        "            \"score\": {\"$meta\": \"vectorSearchScore\"}\n",
        "        }\n",
        "    }\n",
        "  ]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t5Oz5F26DPMJ"
      },
      "outputs": [],
      "source": [
        "# Execute the aggregation pipeline and view the results\n",
        "results = collection.aggregate(pipeline)\n",
        "list(results)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "krlMFHEzDPMJ"
      },
      "source": [
        "### Filter on documents which have been updated on or after `2024-05-19` and where the content type is `Tutorial`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sLXL0PtJDPMJ"
      },
      "outputs": [],
      "source": [
        "# Modify the vector search index `model` (from Step 7) to include the `metadata.contentType` and `updated` fields as `filter` fields\n",
        "model = {\n",
        "    \"name\": ATLAS_VECTOR_SEARCH_INDEX_NAME,\n",
        "    \"type\": \"vectorSearch\",\n",
        "    \"definition\": {\n",
        "        \"fields\": [\n",
        "            {\n",
        "                \"type\": \"vector\",\n",
        "                \"path\": \"embedding\",\n",
        "                \"numDimensions\": 384,\n",
        "                \"similarity\": \"cosine\"\n",
        "            },\n",
        "            {\"type\": \"filter\", \"path\": \"metadata.contentType\"},\n",
        "            {\"type\": \"filter\", \"path\": \"updated\"}\n",
        "        ]\n",
        "    }\n",
        "  }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vj0ODDpDDPMK"
      },
      "outputs": [],
      "source": [
        "# Update vector search index\n",
        "collection.update_search_index(\n",
        "    name=ATLAS_VECTOR_SEARCH_INDEX_NAME, definition=model[\"definition\"]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1TjQgckHDPMK"
      },
      "outputs": [],
      "source": [
        "# Embed the user query\n",
        "query_embedding = get_embedding(\"What is MongoDB Atlas Search?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N1LB_odSDPMK"
      },
      "outputs": [],
      "source": [
        "# Modify the $vectorSearch stage of the aggregation pipeline defined previously to include a filter for documents where\n",
        "# the `metadata.contentType` field has the value \"Tutorial\"\n",
        "# AND\n",
        "# the `updated` field is greater than or equal to \"2024-05-19\"\n",
        "pipeline = [\n",
        "    {\n",
        "        \"$vectorSearch\": {\n",
        "            \"index\": ATLAS_VECTOR_SEARCH_INDEX_NAME,\n",
        "            \"path\": \"embedding\",\n",
        "            \"queryVector\": query_embedding,\n",
        "            \"numCandidates\": 150,\n",
        "            \"limit\": 5,\n",
        "            \"filter\": {\n",
        "                \"$and\": [\n",
        "                    {\"metadata.contentType\": \"Tutorial\"},\n",
        "                    {\"updated\": {\"$gte\": \"2024-05-19\"}}\n",
        "                ]\n",
        "            }\n",
        "        }\n",
        "    },\n",
        "    {\n",
        "        \"$project\": {\n",
        "            \"_id\": 0,\n",
        "            \"body\": 1,\n",
        "            \"updated\": 1,\n",
        "            \"score\": {\"$meta\": \"vectorSearchScore\"}\n",
        "        }\n",
        "    }\n",
        "  ]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5iN0TLg2DPMK"
      },
      "outputs": [],
      "source": [
        "# Execute the aggregation pipeline and view the results\n",
        "results = collection.aggregate(pipeline)\n",
        "list(results)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ua0sqJOADPMK"
      },
      "source": [
        "# Step 9: Build the RAG application\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P3OT_P_DDPMK"
      },
      "source": [
        "### Instantiate a chat model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6NcPurjtDPMK"
      },
      "outputs": [],
      "source": [
        "from fireworks.client import Fireworks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WIQMljyLDPML"
      },
      "outputs": [],
      "source": [
        "# Initializing the Fireworks AI client and the model string\n",
        "fw_client = Fireworks()\n",
        "model = \"accounts/fireworks/models/llama-v3-8b-instruct\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N35fNyHhDPML"
      },
      "source": [
        "### Define a function to create the chat prompt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2Eru4CcfDPML"
      },
      "outputs": [],
      "source": [
        "# Define a function to create the user prompt for our RAG application\n",
        "def create_prompt(user_query: str) -> str:\n",
        "    \"\"\"\n",
        "    Create a chat prompt that includes the user query and retrieved context.\n",
        "\n",
        "    Args:\n",
        "        user_query (str): The user's query string.\n",
        "\n",
        "    Returns:\n",
        "        str: The chat prompt string.\n",
        "    \"\"\"\n",
        "    # Retrieve the most relevant documents for the `user_query` using the `vector_search` function defined in Step 8\n",
        "    context = vector_search(user_query)\n",
        "    # Join the retrieved documents into a single string, where each document is separated by two new lines (\"\\n\\n\")\n",
        "    context = \"\\n\\n\".join([doc.get('body') for doc in context])\n",
        "    # Prompt consisting of the question and relevant context to answer it\n",
        "    prompt = f\"Answer the question based only on the following context. If the context is empty, say I DON'T KNOW\\n\\nContext:\\n{context}\\n\\nQuestion:{user_query}\"\n",
        "    return prompt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yKuJW3QjDPMM"
      },
      "source": [
        "### Define a function to answer user queries\n",
        "\n",
        "ðŸ“š https://docs.fireworks.ai/guides/querying-text-models#chat-completions-api"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0Y7IMLymDPMM"
      },
      "outputs": [],
      "source": [
        "# Define a function to answer user queries using Fireworks' Chat Completion API\n",
        "def generate_answer(user_query: str) -> None:\n",
        "    \"\"\"\n",
        "    Generate an answer to the user query.\n",
        "\n",
        "    Args:\n",
        "        user_query (str): The user's query string.\n",
        "    \"\"\"\n",
        "    # Use the `create_prompt` function above to create a chat prompt\n",
        "    prompt = create_prompt(user_query)\n",
        "    # Use the `prompt` created above to populate the `content` field in the chat message\n",
        "    response = fw_client.chat.completions.create(\n",
        "        model=model,\n",
        "        messages=[{\"role\": \"user\", \"content\": prompt}]\n",
        "    )\n",
        "    # Print the final answer\n",
        "    print(response.choices[0].message.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d0jG5bmoDPMM"
      },
      "source": [
        "### Query the RAG application\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IdIMiaXaDPMM"
      },
      "outputs": [],
      "source": [
        "generate_answer(\"What is MongoDB Atlas Search?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XusIMjJwDPMM"
      },
      "outputs": [],
      "source": [
        "generate_answer(\"What did I just ask you?\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wNGLEsr2DPMN"
      },
      "source": [
        "# ðŸ¦¹â€â™€ï¸ Re-rank retrieved results\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c4wMVbGYDPMN"
      },
      "outputs": [],
      "source": [
        "from sentence_transformers import CrossEncoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BD-_sFn8DPMN"
      },
      "outputs": [],
      "source": [
        "rerank_model = CrossEncoder(\"mixedbread-ai/mxbai-rerank-xsmall-v1\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gQAq4lrMDPMN"
      },
      "source": [
        "ðŸ“š https://huggingface.co/mixedbread-ai/mxbai-rerank-xsmall-v1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6QjKeuqDDPMO"
      },
      "outputs": [],
      "source": [
        "# Add a re-ranking step to the following function\n",
        "def create_prompt(user_query: str) -> str:\n",
        "    \"\"\"\n",
        "    Create a chat prompt that includes the user query and retrieved context.\n",
        "\n",
        "    Args:\n",
        "        user_query (str): The user's query string.\n",
        "\n",
        "    Returns:\n",
        "        str: The chat prompt string.\n",
        "    \"\"\"\n",
        "    # Retrieve the most relevant documents for the `user_query` using the `vector_search` function defined in Step 8\n",
        "    context = vector_search(user_query)\n",
        "    # Extract the \"body\" field from each document in `context`\n",
        "    documents = [d.get(\"body\") for d in context]\n",
        "    # Use the `rerank_model` instantiated above to re-rank `documents`\n",
        "    # Set the `top_k` argument to 5\n",
        "    reranked_documents = rerank_model.rank(\n",
        "        user_query, documents, return_documents=True, top_k=5\n",
        "    )\n",
        "    # Join the re-ranked documents into a single string, where each document is separated by two new lines (\"\\n\\n\")\n",
        "    context = \"\\n\\n\".join([d.get(\"text\", \"\") for d in reranked_documents])\n",
        "    # Prompt consisting of the question and relevant context to answer it\n",
        "    prompt = f\"Answer the question based only on the following context. If the context is empty, say I DON'T KNOW\\n\\nContext:\\n{context}\\n\\nQuestion:{user_query}\"\n",
        "    return prompt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RLIH2iK_DPMO"
      },
      "outputs": [],
      "source": [
        "# Note the impact of re-ranking on the generated answer\n",
        "generate_answer(\"What are triggers in MongoDB Atlas?\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fVJc91HvDPMO"
      },
      "source": [
        "# ðŸ¦¹â€â™€ï¸ Return streaming responses\n",
        "\n",
        "ðŸ“š https://docs.fireworks.ai/guides/querying-text-models#streaming\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k5ftidzKDPMP"
      },
      "outputs": [],
      "source": [
        "# Define a function to answer user queries in streaming mode using Fireworks' Chat Completion API\n",
        "def generate_answer(user_query: str) -> None:\n",
        "    \"\"\"\n",
        "    Generate an answer to the user query.\n",
        "\n",
        "    Args:\n",
        "        user_query (str): The user's query string.\n",
        "    \"\"\"\n",
        "    # Use the `create_prompt` function defined in Step 9 to create a chat prompt\n",
        "    prompt = create_prompt(user_query)\n",
        "    # Use the `prompt` created above to populate the `content` field in the chat message\n",
        "    # Set the `stream` parameter to True\n",
        "    response = fw_client.chat.completions.create(\n",
        "        model=model,\n",
        "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "        stream=True\n",
        "    )\n",
        "\n",
        "    # Iterate through the `response` generator and print the results as they are generated\n",
        "    for chunk in response:\n",
        "      if chunk.choices[0].delta.content:\n",
        "          print(chunk.choices[0].delta.content, end=\"\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u70gNR7wDPMP"
      },
      "outputs": [],
      "source": [
        "generate_answer(\"What is MongoDB Atlas Search?\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5dXcKZkNDPMP"
      },
      "source": [
        "# Step 10: Add memory to the RAG application\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PvrzA2idDPMP"
      },
      "outputs": [],
      "source": [
        "from datetime import datetime"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g0rbzNtfDPMP"
      },
      "outputs": [],
      "source": [
        "history_collection = mongodb_client[DB_NAME][\"chat_history\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "czXG1UnXDPMP"
      },
      "source": [
        "ðŸ“š https://pymongo.readthedocs.io/en/stable/api/pymongo/collection.html#pymongo.collection.Collection.create_index\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mYiyy0JxDPMQ"
      },
      "outputs": [],
      "source": [
        "# Create an index on the key `session_id` for the `history_collection` collection\n",
        "history_collection.create_index(\"session_id\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZCAZn_B1DPMQ"
      },
      "source": [
        "### Define a function to store chat messages in MongoDB\n",
        "\n",
        "ðŸ“š https://pymongo.readthedocs.io/en/stable/api/pymongo/collection.html#pymongo.collection.Collection.insert_one"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w1cS6q74DPMQ"
      },
      "outputs": [],
      "source": [
        "def store_chat_message(session_id: str, role: str, content: str) -> None:\n",
        "    \"\"\"\n",
        "    Store a chat message in a MongoDB collection.\n",
        "\n",
        "    Args:\n",
        "        session_id (str): Session ID of the message.\n",
        "        role (str): Role for the message. One of `system`, `user` or `assistant`.\n",
        "        content (str): Content of the message.\n",
        "    \"\"\"\n",
        "    # Create a message object with `session_id`, `role`, `content` and `timestamp` fields\n",
        "    # `timestamp` should be set the current timestamp\n",
        "    message = {\n",
        "        \"session_id\": session_id,\n",
        "        \"role\": role,\n",
        "        \"content\": content,\n",
        "        \"timestamp\": datetime.now(),\n",
        "    }\n",
        "    # Insert the `message` into the `history_collection` collection\n",
        "    history_collection.insert_one(message)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pV1O0-4uDPMQ"
      },
      "source": [
        "### Define a function to retrieve chat history from MongoDB\n",
        "\n",
        "ðŸ“š https://pymongo.readthedocs.io/en/stable/api/pymongo/collection.html#pymongo.collection.Collection.find\n",
        "\n",
        "ðŸ“š https://pymongo.readthedocs.io/en/stable/api/pymongo/cursor.html#pymongo.cursor.Cursor.sort"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GvuOomebDPMQ"
      },
      "outputs": [],
      "source": [
        "def retrieve_session_history(session_id: str) -> List:\n",
        "    \"\"\"\n",
        "    Retrieve chat message history for a particular session.\n",
        "\n",
        "    Args:\n",
        "        session_id (str): Session ID to retrieve chat message history for.\n",
        "\n",
        "    Returns:\n",
        "        List: List of chat messages.\n",
        "    \"\"\"\n",
        "    # Query the `history_collection` collection for documents where the \"session_id\" field has the value of the input `session_id`\n",
        "    # Sort the results in increasing order of the values in `timestamp` field\n",
        "    cursor =  history_collection.find({\"session_id\": session_id}).sort(\"timestamp\", 1)\n",
        "\n",
        "    if cursor:\n",
        "        # Iterate through the cursor and extract the `role` and `content` field from each entry\n",
        "        # Then format each entry as: {\"role\": <role_value>, \"content\": <content_value>}\n",
        "        messages = [{\"role\": msg[\"role\"], \"content\": msg[\"content\"]} for msg in cursor]\n",
        "    else:\n",
        "        # If cursor is empty, return an empty list\n",
        "        messages = []\n",
        "\n",
        "    return messages"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "89GH6s2gDPMQ"
      },
      "source": [
        "### Handle chat history in the `generate_answer` function\n",
        "\n",
        "ðŸ“š https://docs.fireworks.ai/guides/querying-text-models#chat-completions-api\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HalxjuuyDPMQ"
      },
      "outputs": [],
      "source": [
        "def generate_answer(session_id: str, user_query: str) -> None:\n",
        "    \"\"\"\n",
        "    Generate an answer to the user's query taking chat history into account.\n",
        "\n",
        "    Args:\n",
        "        session_id (str): Session ID to retrieve chat history for.\n",
        "        user_query (str): The user's query string.\n",
        "    \"\"\"\n",
        "    # Initialize list of messages to pass to the chat completion model\n",
        "    messages = []\n",
        "\n",
        "    # Retrieve documents relevant to the user query and convert them to a single string\n",
        "    context = vector_search(user_query)\n",
        "    context = \"\\n\\n\".join([d.get(\"body\", \"\") for d in context])\n",
        "    # Create a system prompt containing the retrieved context\n",
        "    system_message = {\n",
        "        \"role\": \"system\",\n",
        "        \"content\": f\"Answer the question based only on the following context. If the context is empty, say I DON'T KNOW\\n\\nContext:\\n{context}\",\n",
        "    }\n",
        "    # Append the system prompt to the `messages` list\n",
        "    messages.append(system_message)\n",
        "\n",
        "    # Use the `retrieve_session_history` function to retrieve message history from MongoDB for the session ID `session_id`\n",
        "    # And add all messages in the message history to the `messages` list\n",
        "    message_history = <CODE_BLOCK_26>\n",
        "    messages.extend(message_history)\n",
        "\n",
        "    # Format the user message in the format {\"role\": <role_value>, \"content\": <content_value>}\n",
        "    # The role value for user messages must be \"user\"\n",
        "    # And append the user message to the `messages` list\n",
        "    user_message = <CODE_BLOCK_27>\n",
        "    messages.append(user_message)\n",
        "\n",
        "    # Call the chat completions API\n",
        "    response = fw_client.chat.completions.create(model=model, messages=messages)\n",
        "\n",
        "    # Extract the answer from the API response\n",
        "    answer = response.choices[0].message.content\n",
        "\n",
        "    # Use the `store_chat_message` function to store the user message and also the generated answer in the message history collection\n",
        "    # The role value for user messages is \"user\", and \"assistant\" for the generated answer\n",
        "    <CODE_BLOCK_28>\n",
        "\n",
        "    print(answer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9bdsR0EEDPMR"
      },
      "outputs": [],
      "source": [
        "generate_answer(\n",
        "    session_id=\"1\",\n",
        "    user_query=\"What are triggers in MongoDB Atlas?\",\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GDAkv5PbDPMR"
      },
      "outputs": [],
      "source": [
        "generate_answer(\n",
        "    session_id=\"1\",\n",
        "    user_query=\"What did I just ask you?\",\n",
        ")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.1"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}